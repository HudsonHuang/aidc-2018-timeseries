{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Temporal Convolution Network (TCN) for predictive maintenance of aircraft engines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data\n",
    "Use the `TurboFan` class in `datasets.turbofan` to download and pre-process the turbofan dataset.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add aidc-2018-timeseries to notebook path\n",
    "import sys\n",
    "sys.path.append(\"../\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "../datasets/turbofan.py:174: FutureWarning: Method .as_matrix will be removed in a future version. Use .values instead.\n",
      "  trajectories.append(traj[[\"setting_\" + str(i + 1) for i in range(self.n_operating_modes)] + [\"sensor_\" + str(i + 1) for i in range(self.n_sensors)]].as_matrix())\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating sliding window data\n",
      "Done. Number of samples in train: 160854, number of samples in test: 707\n"
     ]
    }
   ],
   "source": [
    "from datasets.turbofan import TurboFan\n",
    "\n",
    "# dataset configuration\n",
    "data_dir = \"./\"  # Location to download data to\n",
    "seq_len = 50     # Sequence length of samples in final processed dataset\n",
    "skip = 1         # Number of time-points between adjacent windows\n",
    "max_rul = 130    # Value at while the 'y' variable is capped\n",
    "\n",
    "# create dataset\n",
    "turbofan_dataset = TurboFan(data_dir=data_dir, T=seq_len, skip=skip, max_rul_predictable=max_rul,\n",
    "                            recurrent_axis_name='W', feature_axis_name='C', label_axis_name='Fo')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Call the `plot_sample` method of the dataset object to visualize a sample set of readings from the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_folder = \"./\"  # Folder to write images to\n",
    "turbofan_dataset.plot_sample(out_folder, trajectory_id=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a data loader in ngraph-neon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ngraph.frontends.neon import ArrayIterator\n",
    "\n",
    "batch_size = 128\n",
    "num_iterations = 10000\n",
    "\n",
    "train_iterator = ArrayIterator(turbofan_dataset.train,\n",
    "                               batch_size,\n",
    "                               total_iterations=num_iterations,\n",
    "                               shuffle=True)\n",
    "val_iterator = ArrayIterator(turbofan_dataset.test, batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define placeholders\n",
    "First, we need to define placeholders; these will be used to feed data into the model in batches.\n",
    "`Axes` are a unique feature of ngraph; they are composed of `Axis` objects which are named dimensions that are matched by name instead of by ordering.\n",
    "Individual `Axis` can be named arbitrarily, though ngraph provides conventions for the batch axis (`'N'`) as \n",
    "well as the time axis in recurrent models and channel and spatial axes in convolutional models.\n",
    "\n",
    "\n",
    "(Typically, the time axis in ngraph is called `'REC'` (recurrent) but because we are using a convolutional model\n",
    "to process time series data, the time axis maps to the `'W'` axis, and the feature axis corresponds to `'C'`, the\n",
    "convolution channels.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ngraph as ng\n",
    "\n",
    "# number of input and output features  \n",
    "n_features = turbofan_dataset.train['X']['data'].shape[2] # read off the number of features in the input data\n",
    "n_output_features = 1\n",
    "\n",
    "# name and create axes\n",
    "batch_axis = ng.make_axis(length=batch_size, name=\"N\")\n",
    "time_axis = ng.make_axis(length=seq_len, name=\"W\")\n",
    "feature_axis = ng.make_axis(length=n_features, name=\"C\")\n",
    "out_axis = ng.make_axis(length=n_output_features, name=\"Fo\")\n",
    "\n",
    "in_axes = ng.make_axes([batch_axis, time_axis, feature_axis])\n",
    "out_axes = ng.make_axes([batch_axis, out_axis])\n",
    "\n",
    "# build placeholders for the created axes\n",
    "inputs = dict(X=ng.placeholder(in_axes), \n",
    "              y=ng.placeholder(out_axes))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build TCN Model\n",
    "\n",
    "The TCN model consists of residual blocks of which contain dilated convolutional layers, weight normalization, relu activations, and spatial dropout. Let's build a model with a single residual block."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "%%html \n",
    "<table class=\"image\">\n",
    "<caption align=\"bottom\">Residual block</caption>\n",
    "<tr><img src='img/residual_block.png' alt='residual block' width='250'></tr></table>\n",
    "Bai et al \"An Empirical Evaluation of Generic Convolutional and Recurrent Networks for Sequence Modeling\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's first build the main path of the residual block. \n",
    "We create two convolution and dropout layers, and wrap them in a `Sequential` container.\n",
    "`Sequential` takes a list of neon layers or callables. [termininology]\n",
    "Calling a `Sequential` object will cause each of the layers to be called in order."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from topologies.custom_neon_classes import Dropout2D, DilatedCausalConv\n",
    "from ngraph.frontends.neon import GaussianInit, Rectlin, Sequential\n",
    "\n",
    "kernel_size = 5  # conv kernel size\n",
    "n_filters = 50   # number of filters per conv layer\n",
    "dilation = 1     # conv dilation\n",
    "dropout = 0.2    # dropout rate\n",
    "keep = 1 - dropout  # ratio to keep\n",
    "\n",
    "layers = []\n",
    "\n",
    "for i in range(2):\n",
    "    \n",
    "    # causal conv, weight norm, relu\n",
    "    layers += [DilatedCausalConv(filter_shape=(kernel_size, n_filters),\n",
    "                                 padding='causal',\n",
    "                                 dilation=dilation,\n",
    "                                 strides=1,\n",
    "                                 activation=Rectlin(),\n",
    "                                 filter_init=GaussianInit(0, 0.01), \n",
    "                                 batch_norm=False)]\n",
    "\n",
    "    # spatial dropout\n",
    "    layers += [Dropout2D(keep)]\n",
    "\n",
    "# combine layers in a Sequential container\n",
    "main_path = Sequential(layers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now for the side path: this is the residual path. \n",
    "If the number of input channels and output channels of the main path are the same, the residual path passes the input, unmodified, to be added to the output of the main path.\n",
    "Otherwise, it uses 1-dimensional convolutions to ensure that the main and residual paths have the same number of output channels (`n_filters`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ngraph.frontends.neon import Convolution\n",
    "\n",
    "side_path = Sequential([Convolution(filter_shape=(1, n_filters),\n",
    "                                    filter_init=GaussianInit(0, 0.01),\n",
    "                                    strides=1,\n",
    "                                    dilation=1,\n",
    "                                    padding='same',\n",
    "                                    batch_norm=False)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bring the two paths together using the `ResidualModule` API:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ngraph.frontends.neon.model import ResidualModule\n",
    "\n",
    "res_block = ResidualModule(main_path, side_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can combine the code from the last three cells into a factory function that creates residual blocks:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ngraph.frontends.neon import Convolution\n",
    "from ngraph.frontends.neon import GaussianInit, Rectlin, Sequential\n",
    "from ngraph.frontends.neon.model import ResidualModule\n",
    "from topologies.custom_neon_classes import Dropout2D, DilatedCausalConv\n",
    "\n",
    "\n",
    "def residual_block(kernel_size, n_filters, dilation, dropout, res_path_conv=False):\n",
    "\n",
    "    keep = 1 - dropout\n",
    "    \n",
    "    # main path\n",
    "    layers = []\n",
    "    for i in range(2):\n",
    "\n",
    "        # causal conv, weight norm, relu\n",
    "        layers += [DilatedCausalConv(filter_shape=(kernel_size, n_filters),\n",
    "                                     padding='causal',\n",
    "                                     dilation=dilation,\n",
    "                                     strides=1,\n",
    "                                     activation=Rectlin(),\n",
    "                                     filter_init=GaussianInit(0, 0.01), \n",
    "                                     batch_norm=False)]\n",
    "\n",
    "        # spatial dropout  \n",
    "        layers += [Dropout2D(keep)]\n",
    "\n",
    "    # combine layers in a Sequential container\n",
    "    main_path = Sequential(layers)\n",
    "    \n",
    "    # side path\n",
    "    if res_path_conv:\n",
    "        side_path = Sequential([Convolution(filter_shape=(1, n_filters),\n",
    "                                            filter_init=GaussianInit(0, 0.01),\n",
    "                                            strides=1,\n",
    "                                            dilation=1,\n",
    "                                            padding='same',\n",
    "                                            batch_norm=False)])\n",
    "    else:\n",
    "        side_path = None\n",
    "        \n",
    "    # residual block\n",
    "    res_block = ResidualModule(main_path, side_path)\n",
    "    \n",
    "    return res_block"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now build a TCN with a stack of residual blocks, where the convolutional layers in each block have the same number of filters. The first residual block will have 1-D convolutions in the residual path but the subsequent blocks will will not. Here we define the TCN layers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "kernel_size = 4  # conv kernel size\n",
    "n_filters = 50   # number of filters per conv layer\n",
    "dropout = 0.1    # dropout rate\n",
    "\n",
    "num_blocks = 4   # number of residual blocks\n",
    "\n",
    "layers = []\n",
    "for i in range(num_blocks):\n",
    "    \n",
    "    dilation = 2 ** i\n",
    "    \n",
    "    res_path_conv = True if i == 0 else False\n",
    "    layers += [residual_block(kernel_size, n_filters, dilation, dropout, res_path_conv)]\n",
    "    layers += [Rectlin()]\n",
    "        \n",
    "tcn = Sequential(layers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the output of the TCN has the same sequence length as its input; \n",
    "however due to dilation of the convolution filters, the \"receptive field\" of each time-point in the output is much larger. \n",
    "Each time-point operates on multiple past time-points. \n",
    "If the network is deep enough, the last time-point has sufficiently large receptive field that its use alone will suffice to generate the output. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "%%html \n",
    "<table class=\"image\">\n",
    "<caption>Dilated convolution (3 layers)</caption>\n",
    "<tr><img src='img/dilated_conv.png' alt='dilated convolution' height='120'></tr></table>\n",
    "Bai et al \"An Empirical Evaluation of Generic Convolutional and Recurrent Networks for Sequence Modeling\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can read off the last time-point as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def last_timepoint(op):\n",
    "    # select the last timepoint of the Axis named \"W\"\n",
    "    slices = [slice(seq_len-1, seq_len, 1) if ax.name == \"W\" else slice(None) for ax in op.axes]\n",
    "    return ng.tensor_slice(op, slices)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use an `Affine` layer to predict the RUL from the last time-point:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ngraph.frontends.neon.layer import Affine\n",
    "\n",
    "affine_layer = Affine(axes=out_axis, weight_init=GaussianInit(0, 0.01), activation=Rectlin())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, encapsulate the entire model in a `Sequential` container. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential([tcn, \n",
    "                    last_timepoint, \n",
    "                    affine_layer])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Specify training loss, validation loss, and optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Forward propagation through the defined model, on the input placeholder defined above:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_train = model(inputs['X'])  # model output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the training loss as the Mean Squared Error (MSE) between model output `y_hat` and label placeholder:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loss = ng.squared_L2(prediction_train - inputs['y'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similarly, with `Layer.inference_mode_on()`, define the MSE for validation data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ngraph.frontends.neon import Layer\n",
    "\n",
    "with Layer.inference_mode_on():\n",
    "    prediction_val = model(inputs['X'])\n",
    "    \n",
    "eval_loss = ng.mean(ng.squared_L2(prediction_val - inputs['y']), out_axes=())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the optimizer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ngraph.frontends.neon import Adam\n",
    "\n",
    "lr = 0.002                # learning rate\n",
    "grad_clip_value = 0.4     # gradient clip value\n",
    "\n",
    "optimizer = Adam(learning_rate=lr, gradient_clip_value=grad_clip_value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we call the `optimizer`, which causes gradients to be computed and trainable parameters to be updated. \n",
    "We also compute the average minibatch training loss. \n",
    "Note that these two operations are combined into a single, ordered operation using `ng.sequential`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_cost = ng.sequential([optimizer(train_loss), \n",
    "                            ng.mean(train_loss, out_axes=())])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the train and validation computations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_computation = ng.computation(batch_cost, \"all\") \n",
    "eval_computation = ng.computation(eval_loss, \"all\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tensorboard visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ngraph supports Tensorboard for visualization. \n",
    "The code below will cause a directory `tensorboard` to be created in the current directory.\n",
    "It will also create a subdirectory within `tensorboard` named with the current timestamp, e.g. `180521T170237`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ngraph.op_graph.tensorboard.tensorboard import TensorBoard\n",
    "\n",
    "tensorboard_dir = \"./tensorboard/\"  # directory to save tensorboard summaries to\n",
    "\n",
    "# create tensorboard object \n",
    "tb = TensorBoard(tensorboard_dir)\n",
    "\n",
    "# add model graph for visualization\n",
    "tb.add_graph(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To start tensorboard open a terminal and type the commands below.\n",
    "(Remember to replace the timestamp directory name withwhat you see in the local `tensorboard` directory.)\n",
    "Open a browser window and navigate to the URL printed in the terminal."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "cd aidc-2918-timeseries\n",
    ". .venv/bin/activate\n",
    "tensorboard --logdir=./tutorials/tensorboard/180521T170237\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, model training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Could not load MKLDNN Engine:  /Users/rubypai/code/ngraph-python/ngraph/transformers/../../mkldnn_engine.so  Will default to numpy\n",
      "100 Train Loss: 2362.64, Validation Loss: 2523.46\n",
      "200 Train Loss: 1944.61, Validation Loss: 1858.86\n",
      "300 Train Loss: 1633.42, Validation Loss: 1779.18\n",
      "400 Train Loss: 1544.78, Validation Loss: 1579.62\n",
      "500 Train Loss: 2051.88, Validation Loss: 2153.61\n",
      "600 Train Loss: 1909.80, Validation Loss: 1422.49\n",
      "700 Train Loss: 1242.36, Validation Loss: 1460.07\n",
      "800 Train Loss: 2988.52, Validation Loss: 1484.22\n",
      "900 Train Loss: 1494.15, Validation Loss: 1604.00\n",
      "1000 Train Loss: 3087.31, Validation Loss: 1430.84\n",
      "1100 Train Loss: 2107.30, Validation Loss: 1386.80\n",
      "1200 Train Loss: 1528.38, Validation Loss: 1401.26\n",
      "1300 Train Loss: 1375.18, Validation Loss: 1321.00\n",
      "1400 Train Loss: 1990.11, Validation Loss: 1320.74\n",
      "1500 Train Loss: 1411.35, Validation Loss: 2405.94\n",
      "1600 Train Loss: 1214.69, Validation Loss: 1354.75\n",
      "1700 Train Loss: 1188.17, Validation Loss: 1376.87\n",
      "1800 Train Loss: 1258.20, Validation Loss: 1346.09\n",
      "1900 Train Loss: 1540.92, Validation Loss: 1317.57\n",
      "2000 Train Loss: 1072.82, Validation Loss: 1651.90\n",
      "2100 Train Loss: 1425.37, Validation Loss: 1302.51\n",
      "2200 Train Loss: 1176.07, Validation Loss: 1353.07\n",
      "2300 Train Loss: 1172.01, Validation Loss: 1780.17\n",
      "2400 Train Loss: 1462.38, Validation Loss: 1312.13\n",
      "2500 Train Loss: 1925.01, Validation Loss: 1485.14\n",
      "2600 Train Loss: 1169.92, Validation Loss: 1615.57\n",
      "2700 Train Loss: 1429.40, Validation Loss: 1239.41\n",
      "2800 Train Loss: 964.30, Validation Loss: 1045.55\n",
      "2900 Train Loss: 1354.76, Validation Loss: 1179.52\n",
      "3000 Train Loss: 931.47, Validation Loss: 1300.45\n",
      "3100 Train Loss: 1006.17, Validation Loss: 965.19\n",
      "3200 Train Loss: 1104.68, Validation Loss: 906.35\n",
      "3300 Train Loss: 713.57, Validation Loss: 899.26\n",
      "3400 Train Loss: 840.07, Validation Loss: 894.96\n",
      "3500 Train Loss: 788.20, Validation Loss: 815.88\n",
      "3600 Train Loss: 900.98, Validation Loss: 777.87\n",
      "3700 Train Loss: 658.17, Validation Loss: 861.17\n",
      "3800 Train Loss: 768.24, Validation Loss: 893.43\n",
      "3900 Train Loss: 871.17, Validation Loss: 767.30\n",
      "4000 Train Loss: 766.68, Validation Loss: 1070.12\n",
      "4100 Train Loss: 465.91, Validation Loss: 790.57\n",
      "4200 Train Loss: 1805.60, Validation Loss: 1052.39\n",
      "4300 Train Loss: 807.39, Validation Loss: 754.70\n",
      "4400 Train Loss: 1075.29, Validation Loss: 821.98\n",
      "4500 Train Loss: 543.83, Validation Loss: 742.89\n",
      "4600 Train Loss: 929.19, Validation Loss: 755.00\n",
      "4700 Train Loss: 540.15, Validation Loss: 808.71\n",
      "4800 Train Loss: 936.50, Validation Loss: 758.97\n",
      "4900 Train Loss: 660.89, Validation Loss: 811.39\n",
      "5000 Train Loss: 610.27, Validation Loss: 719.47\n",
      "5100 Train Loss: 449.57, Validation Loss: 719.79\n",
      "5200 Train Loss: 426.79, Validation Loss: 697.18\n",
      "5300 Train Loss: 570.73, Validation Loss: 792.85\n",
      "5400 Train Loss: 585.67, Validation Loss: 714.46\n",
      "5500 Train Loss: 609.26, Validation Loss: 777.72\n",
      "5600 Train Loss: 1008.52, Validation Loss: 973.86\n",
      "5700 Train Loss: 417.26, Validation Loss: 739.46\n",
      "5800 Train Loss: 393.46, Validation Loss: 685.81\n",
      "5900 Train Loss: 761.71, Validation Loss: 661.79\n",
      "6000 Train Loss: 885.77, Validation Loss: 704.52\n",
      "6100 Train Loss: 362.63, Validation Loss: 792.79\n",
      "6200 Train Loss: 533.74, Validation Loss: 621.32\n",
      "6300 Train Loss: 575.42, Validation Loss: 687.34\n",
      "6400 Train Loss: 513.01, Validation Loss: 731.34\n",
      "6500 Train Loss: 327.39, Validation Loss: 650.02\n",
      "6600 Train Loss: 541.76, Validation Loss: 712.78\n",
      "6700 Train Loss: 694.38, Validation Loss: 748.97\n",
      "6800 Train Loss: 620.30, Validation Loss: 681.19\n",
      "6900 Train Loss: 665.83, Validation Loss: 832.53\n",
      "7000 Train Loss: 503.21, Validation Loss: 640.42\n",
      "7100 Train Loss: 581.81, Validation Loss: 653.53\n",
      "7200 Train Loss: 640.77, Validation Loss: 758.80\n",
      "7300 Train Loss: 489.16, Validation Loss: 620.10\n",
      "7400 Train Loss: 559.20, Validation Loss: 579.73\n",
      "7500 Train Loss: 419.34, Validation Loss: 542.96\n",
      "7600 Train Loss: 520.17, Validation Loss: 603.08\n",
      "7700 Train Loss: 382.58, Validation Loss: 627.07\n",
      "7800 Train Loss: 451.45, Validation Loss: 641.35\n",
      "7900 Train Loss: 428.70, Validation Loss: 722.18\n",
      "8000 Train Loss: 507.17, Validation Loss: 584.99\n",
      "8100 Train Loss: 479.59, Validation Loss: 630.28\n",
      "8200 Train Loss: 451.50, Validation Loss: 624.32\n",
      "8300 Train Loss: 372.74, Validation Loss: 541.75\n",
      "8400 Train Loss: 512.25, Validation Loss: 620.71\n",
      "8500 Train Loss: 372.43, Validation Loss: 544.25\n",
      "8600 Train Loss: 442.42, Validation Loss: 572.03\n",
      "8700 Train Loss: 389.11, Validation Loss: 562.99\n",
      "8800 Train Loss: 375.74, Validation Loss: 544.19\n",
      "8900 Train Loss: 444.76, Validation Loss: 604.90\n",
      "9000 Train Loss: 772.58, Validation Loss: 558.41\n",
      "9100 Train Loss: 326.82, Validation Loss: 621.88\n",
      "9200 Train Loss: 886.12, Validation Loss: 535.99\n",
      "9300 Train Loss: 306.18, Validation Loss: 625.54\n",
      "9400 Train Loss: 540.48, Validation Loss: 613.58\n",
      "9500 Train Loss: 487.84, Validation Loss: 553.08\n",
      "9600 Train Loss: 530.65, Validation Loss: 541.38\n",
      "9700 Train Loss: 437.47, Validation Loss: 528.96\n",
      "9800 Train Loss: 386.42, Validation Loss: 545.84\n",
      "9900 Train Loss: 400.45, Validation Loss: 532.33\n"
     ]
    }
   ],
   "source": [
    "import ngraph.transformers as ngt\n",
    "from contextlib import closing\n",
    "\n",
    "train_iterator.reset()  # not necessary the first time train loop is run\n",
    "\n",
    "# Now bind the computations we are interested in\n",
    "with closing(ngt.make_transformer()) as transformer:\n",
    "    \n",
    "    # compile computations\n",
    "    train_function = transformer.add_computation(train_computation)\n",
    "    eval_function = transformer.add_computation(eval_computation)\n",
    "    \n",
    "    # train loop\n",
    "    for step, data in enumerate(train_iterator):\n",
    "\n",
    "        # construct feed dictionary of inputs to train function\n",
    "        feed_dict = {inputs[\"X\"]: data[\"X\"], inputs[\"y\"]: data[\"y\"]}\n",
    "        \n",
    "        # Mean batch cost\n",
    "        output = train_function(feed_dict=feed_dict)\n",
    "        train_loss = output[()].item()\n",
    "        tb.add_scalar(\"train_loss\", train_loss, step=step)\n",
    "        \n",
    "        # Every N iterations print test set metrics\n",
    "        if step % 100 == 0 and step > 0:\n",
    "\n",
    "            # calculate metrics over test set\n",
    "            avg_eval_loss = 0.0\n",
    "            val_iterator.reset()\n",
    "            for e, data_test in enumerate(val_iterator):\n",
    "                feed_dict_test = {inputs[\"X\"]: data_test[\"X\"], inputs[\"y\"]: data_test[\"y\"]}\n",
    "                eval_loss = eval_function(feed_dict=feed_dict_test)\n",
    "                avg_eval_loss += eval_loss\n",
    "\n",
    "            avg_eval_loss /= (e + 1)\n",
    "            tb.add_scalar(\"eval_loss\", avg_eval_loss, step=step)\n",
    "            \n",
    "            print(\"%d Train Loss: %4.2f, Validation Loss: %4.2f\" % (step, train_loss, avg_eval_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
