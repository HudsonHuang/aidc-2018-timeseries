{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Temporal Convolution Network (TCN) for predictive maintenance of aircraft engines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data\n",
    "Use the `TurboFan` class in `datasets.turbofan` to download and pre-process the turbofan dataset.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# use within timeseries repo\n",
    "import sys\n",
    "sys.path.append(\"../\")\n",
    "\n",
    "from datasets.turbofan import TurboFan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data\n",
      "Creating sliding window data\n",
      "Done. Number of samples in train: 160854, number of samples in test: 707\n"
     ]
    }
   ],
   "source": [
    "# from turbofan import TurboFan\n",
    "\n",
    "# dataset configuration\n",
    "data_dir = \"./\"  # Location to download data to\n",
    "seq_len = 50     # Sequence length of samples in final processed dataset\n",
    "skip = 1         # Number of time-points to skip over while processing (i.e., number of time-points between adjacent windows)\n",
    "max_rul = 130    # Value at while the 'y' variable is capped\n",
    "\n",
    "# create dataset\n",
    "turbofan_dataset = TurboFan(data_dir=data_dir, T=seq_len, skip=skip, max_rul_predictable=max_rul,\n",
    "                            recurrent_axis_name='W', feature_axis_name='C', label_axis_name='Fo')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Call the `plot_sample` method of the dataset object to visualize a sample set of readings from the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "out_folder = \"./\"  # Folder to write images to\n",
    "turbofan_dataset.plot_sample(out_folder, trajectory_id=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a data loader in ngraph-neon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from ngraph.frontends.neon import ArrayIterator\n",
    "\n",
    "batch_size = 128\n",
    "num_iterations = 10000\n",
    "\n",
    "train_iterator = ArrayIterator(turbofan_dataset.train,\n",
    "                               batch_size,\n",
    "                               total_iterations=num_iterations,\n",
    "                               shuffle=True)\n",
    "val_iterator = ArrayIterator(turbofan_dataset.test, batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define placeholders\n",
    "First, we need to define placeholders; these will be used to feed data into the model in batches.\n",
    "`Axes` are a unique feature of ngraph; they are composed of `Axis` objects which are named dimensions that are matched by name instead of by ordering.\n",
    "Individual `Axis` can be named arbitrarily, though ngraph provides conventions for the batch axis (`'N'`) as \n",
    "well as the time axis in recurrent models and channel and spatial axes in convolutional models.\n",
    "\n",
    "\n",
    "(Typically, the time axis in ngraph is called `'REC'` (recurrent) but because we are using a convolutional model\n",
    "to process time series data, the time axis maps to the `'W'` axis, and the feature axis corresponds to `'C'`, the\n",
    "convolution channels.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import ngraph as ng\n",
    "\n",
    "# number of input and output features  \n",
    "n_features = turbofan_dataset.train['X']['data'].shape[2] # read off the number of features in the input data\n",
    "n_output_features = 1\n",
    "\n",
    "# name and create axes\n",
    "batch_axis = ng.make_axis(length=batch_size, name=\"N\")\n",
    "time_axis = ng.make_axis(length=seq_len, name=\"W\")\n",
    "feature_axis = ng.make_axis(length=n_features, name=\"C\")\n",
    "out_axis = ng.make_axis(length=n_output_features, name=\"Fo\")\n",
    "\n",
    "in_axes = ng.make_axes([batch_axis, time_axis, feature_axis])\n",
    "out_axes = ng.make_axes([batch_axis, out_axis])\n",
    "\n",
    "# build placeholders for the created axes\n",
    "inputs = dict(X=ng.placeholder(in_axes), \n",
    "              y=ng.placeholder(out_axes))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build TCN Model\n",
    "\n",
    "The TCN model consists of residual blocks of which contain dilated convolutional layers, weight normalization, relu activations, and spatial dropout. Let's build a model with a single residual block."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "%%html \n",
    "<table class=\"image\">\n",
    "<caption align=\"bottom\">Residual block</caption>\n",
    "<tr><img src='img/residual_block.png' alt='residual block' width='250'></tr></table>\n",
    "Bai et al \"An Empirical Evaluation of Generic Convolutional and Recurrent Networks for Sequence Modeling\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's first build the main path of the residual block. \n",
    "We create two convolution and dropout layers, and wrap them in a `Sequential` container.\n",
    "`Sequential` takes a list of neon layers or callables. [termininology]\n",
    "Calling a `Sequential` object will cause each of the layers to be called in order."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from ngraph.frontends.neon.layer import Convolution, Dropout2D\n",
    "from ngraph.frontends.neon import GaussianInit, Rectlin, Sequential\n",
    "\n",
    "kernel_size = 5  # conv kernel size\n",
    "n_filters = 50   # number of filters per conv layer\n",
    "stride = 1       # conv stride\n",
    "dilation = 1     # conv dilation\n",
    "dropout = 0.2    # dropout rate\n",
    "keep = 1 - dropout  # ratio to keep\n",
    "\n",
    "layers = []\n",
    "\n",
    "for i in range(2):\n",
    "    \n",
    "    # causal conv, weight norm, relu\n",
    "    layers += [Convolution(filter_shape=(kernel_size, n_filters),\n",
    "                             padding='causal',\n",
    "                             dilation=dilation, \n",
    "                             strides=stride, \n",
    "                             filter_init=GaussianInit(0, 0.01), \n",
    "                             batch_norm=False, \n",
    "                             weight_norm=True,\n",
    "                             activation=Rectlin())]\n",
    "\n",
    "    # spatial dropout\n",
    "    layers += [Dropout2D(keep)]\n",
    "\n",
    "# combine layers in a Sequential container\n",
    "main_path = Sequential(layers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now for the side path: this is the residual path. \n",
    "If the number of input channels and output channels of the main path are the same, the residual path passes the input, unmodified, to be added to the output of the main path.\n",
    "Otherwise, it uses 1-dimensional convolutions to ensure that the main and residual paths have the same number of output channels (`n_filters`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "side_path = Sequential([Convolution(filter_shape=(1, n_filters),\n",
    "                                    filter_init=GaussianInit(0, 0.01),\n",
    "                                    strides=1,\n",
    "                                    dilation=1,\n",
    "                                    padding='same',\n",
    "                                    batch_norm=False)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bring the two paths together using the `ResidualModule` API:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from ngraph.frontends.neon.model import ResidualModule\n",
    "\n",
    "res_block = ResidualModule(main_path, side_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can combine the code from the last three cells into a factory function that creates residual blocks:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from ngraph.frontends.neon.layer import Convolution, Dropout2D\n",
    "from ngraph.frontends.neon import GaussianInit, Rectlin, Sequential\n",
    "from ngraph.frontends.neon.model import ResidualModule\n",
    "\n",
    "\n",
    "def residual_block(kernel_size, n_filters, dilation, dropout, res_path_conv=False):\n",
    "\n",
    "    keep = 1 - dropout\n",
    "    \n",
    "    # main path\n",
    "    layers = []\n",
    "    for i in range(2):\n",
    "\n",
    "        # causal conv, weight norm, relu\n",
    "        layers += [Convolution(filter_shape=(kernel_size, n_filters),\n",
    "                                 padding='causal',\n",
    "                                 dilation=dilation, \n",
    "                                 strides=1, \n",
    "                                 filter_init=GaussianInit(0, 0.01), \n",
    "                                 batch_norm=False, \n",
    "                                 weight_norm=True,\n",
    "                                 activation=Rectlin())]\n",
    "\n",
    "        # spatial dropout  \n",
    "        layers += [Dropout2D(keep)]\n",
    "\n",
    "    # combine layers in a Sequential container\n",
    "    main_path = Sequential(layers)\n",
    "    \n",
    "    # side path\n",
    "    if res_path_conv:\n",
    "        side_path = Sequential([Convolution(filter_shape=(1, n_filters),\n",
    "                                    filter_init=GaussianInit(0, 0.01),\n",
    "                                    strides=1,\n",
    "                                    dilation=1,\n",
    "                                    padding='same',\n",
    "                                    batch_norm=False)])\n",
    "    else:\n",
    "        side_path = None\n",
    "        \n",
    "    # residual block\n",
    "    res_block = ResidualModule(main_path, side_path)\n",
    "    \n",
    "    return res_block"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now build a TCN with a stack of residual blocks, where the convolutional layers in each block have the same number of filters. The first residual block will have 1-D convolutions in the residual path but the subsequent blocks will will not. Here we define the TCN layers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "kernel_size = 4  # conv kernel size\n",
    "n_filters = 50   # number of filters per conv layer\n",
    "dropout = 0.1    # dropout rate\n",
    "\n",
    "num_blocks = 4   # number of residual blocks\n",
    "\n",
    "layers = []\n",
    "for i in range(num_blocks):\n",
    "    \n",
    "    dilation = 2 ** i\n",
    "    \n",
    "    res_path_conv = True if i == 0 else False\n",
    "    layers += [residual_block(kernel_size, n_filters, dilation, dropout, res_path_conv)]\n",
    "    layers += [Rectlin()]\n",
    "        \n",
    "tcn = Sequential(layers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the output of the TCN has the same sequence length as its input; \n",
    "however due to dilation of the convolution filters, the \"receptive field\" of each time-point in the output is much larger. \n",
    "Each time-point operates on multiple past time-points. \n",
    "If the network is deep enough, the last time-point has sufficiently large receptive field that its use alone will suffice to generate the output. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "%%html \n",
    "<table class=\"image\">\n",
    "<caption>Dilated convolution (3 layers)</caption>\n",
    "<tr><img src='img/dilated_conv.png' alt='dilated convolution' height='120'></tr></table>\n",
    "Bai et al \"An Empirical Evaluation of Generic Convolutional and Recurrent Networks for Sequence Modeling\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can read off the last time-point as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def last_timepoint(op):\n",
    "    # select the last timepoint of the Axis named \"W\"\n",
    "    slices = [slice(seq_len-1, seq_len, 1) if ax.name == \"W\" else slice(None) for ax in op.axes]\n",
    "    return ng.tensor_slice(op, slices)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use an `Affine` layer to predict the RUL from the last time-point:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from ngraph.frontends.neon.layer import Affine\n",
    "\n",
    "affine_layer = Affine(axes=out_axis, weight_init=GaussianInit(0, 0.01), activation=Rectlin())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, encapsulate the entire model in a `Sequential` container. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model = Sequential([tcn, \n",
    "                    last_timepoint, \n",
    "                    affine_layer])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Specify training loss, validation loss, and optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Forward propagation through the defined model, on the input placeholder defined above:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "prediction_train = model(inputs['X'])  # model output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the training loss as the Mean Squared Error (MSE) between model output `y_hat` and label placeholder:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_loss = ng.squared_L2(prediction_train - inputs['y'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similarly, with `Layer.inference_mode_on()`, define the MSE for validation data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from ngraph.frontends.neon import Layer\n",
    "\n",
    "with Layer.inference_mode_on():\n",
    "    prediction_val = model(inputs['X'])\n",
    "    \n",
    "eval_loss = ng.mean(ng.squared_L2(prediction_val - inputs['y']), out_axes=())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the optimizer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from ngraph.frontends.neon import Adam\n",
    "\n",
    "lr = 0.002                # learning rate\n",
    "grad_clip_value = 0.4     # gradient clip value\n",
    "\n",
    "optimizer = Adam(learning_rate=lr, gradient_clip_value=grad_clip_value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we call the `optimizer`, which causes gradients to be computed and trainable parameters to be updated. \n",
    "We also compute the average minibatch training loss. \n",
    "Note that these two operations are combined into a single, ordered operation using `ng.sequential`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_cost = ng.sequential([optimizer(train_loss), \n",
    "                            ng.mean(train_loss, out_axes=())])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the train and validation computations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_computation = ng.computation(batch_cost, \"all\") \n",
    "eval_computation = ng.computation(eval_loss, \"all\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ngraph supports Tensorboard for visualization:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from ngraph.op_graph.tensorboard.tensorboard import TensorBoard\n",
    "\n",
    "tensorboard_dir = \"./tensorboard/\"  # directory to save tensorboard summaries to\n",
    "\n",
    "# create tensorboard object \n",
    "tb = TensorBoard(tensorboard_dir)\n",
    "\n",
    "# add model graph for visualization\n",
    "tb.add_graph(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, model training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Could not load MKLDNN Engine:  /Users/rubypai/code/private-ngraph/ngraph/transformers/../../mkldnn_engine.so  Will default to numpy\n",
      "100 Train Loss: 2390.51, Validation Loss: 2638.68\n",
      "200 Train Loss: 2072.15, Validation Loss: 1689.25\n",
      "300 Train Loss: 1638.39, Validation Loss: 2006.12\n",
      "400 Train Loss: 2243.66, Validation Loss: 1528.12\n",
      "500 Train Loss: 1465.33, Validation Loss: 1828.99\n",
      "600 Train Loss: 2929.62, Validation Loss: 2149.43\n",
      "700 Train Loss: 1399.94, Validation Loss: 1574.89\n",
      "800 Train Loss: 1508.14, Validation Loss: 1385.97\n",
      "900 Train Loss: 1608.22, Validation Loss: 1484.85\n",
      "1000 Train Loss: 1320.38, Validation Loss: 1365.58\n"
     ]
    }
   ],
   "source": [
    "import ngraph.transformers as ngt\n",
    "from contextlib import closing\n",
    "\n",
    "train_iterator.reset()  # not necessary the first time train loop is run\n",
    "\n",
    "# Now bind the computations we are interested in\n",
    "with closing(ngt.make_transformer()) as transformer:\n",
    "    \n",
    "    # compile computations\n",
    "    train_function = transformer.add_computation(train_computation)\n",
    "    eval_function = transformer.add_computation(eval_computation)\n",
    "    \n",
    "    # train loop\n",
    "    for step, data in enumerate(train_iterator):\n",
    "\n",
    "        # construct feed dictionary of inputs to train function\n",
    "        feed_dict = {inputs[\"X\"]: data[\"X\"], inputs[\"y\"]: data[\"y\"]}\n",
    "        \n",
    "        # Mean batch cost\n",
    "        output = train_function(feed_dict=feed_dict)\n",
    "        train_loss = output[()].item()\n",
    "        tb.add_scalar(\"train_loss\", train_loss, step=step)\n",
    "        \n",
    "        # Every N iterations print test set metrics\n",
    "        if step % 100 == 0 and step > 0:\n",
    "\n",
    "            # calculate metrics over test set\n",
    "            avg_eval_loss = 0.0\n",
    "            val_iterator.reset()\n",
    "            for e, data_test in enumerate(val_iterator):\n",
    "                feed_dict_test = {inputs[\"X\"]: data_test[\"X\"], inputs[\"y\"]: data_test[\"y\"]}\n",
    "                eval_loss = eval_function(feed_dict=feed_dict_test)\n",
    "                avg_eval_loss += eval_loss\n",
    "\n",
    "            avg_eval_loss /= (e + 1)\n",
    "            tb.add_scalar(\"eval_loss\", avg_eval_loss, step=step)\n",
    "            \n",
    "            print(\"%d Train Loss: %4.2f, Validation Loss: %4.2f\" % (step, train_loss, avg_eval_loss))"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
